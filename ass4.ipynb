{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jazzblazzer/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import torch.nn as nn\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "nltk.download('stopwords')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader\n",
    "from torchtext.vocab import build_vocab_from_iterator, Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unions representing workers at Turner   Newall say they are 'disappointed' after talks with stricken parent firm Federal Mogul.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('iNLP-A2/ANLP-2/test.csv')\n",
    "\n",
    "# Display the data\n",
    "print(df['Description'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TOKEN = \"<s>\"\n",
    "END_TOKEN = \"</s>\"\n",
    "UNKNOWN_TOKEN = \"<unk>\"\n",
    "PAD_TOKEN = \"<pad>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['<s>', 'Unions', 'representing', 'workers', 'at', 'Turner', 'Newall', 'say', 'they', 'are', \"'disappointed\", \"'\", 'after', 'talks', 'with', 'stricken', 'parent', 'firm', 'Federal', 'Mogul', '.'], ['Unions', 'representing', 'workers', 'at', 'Turner', 'Newall', 'say', 'they', 'are', \"'disappointed\", \"'\", 'after', 'talks', 'with', 'stricken', 'parent', 'firm', 'Federal', 'Mogul', '.', '</s>'])\n"
     ]
    }
   ],
   "source": [
    "class LSTMDataset:\n",
    "    def __init__(self, dataset, tokenizer, max_length):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        tokenized_sentence = self.tokenizer(self.dataset[idx])\n",
    "        return [START_TOKEN]+tokenized_sentence,tokenized_sentence+[END_TOKEN]\n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "lstm_data = LSTMDataset(df['Description'],nltk.word_tokenize, 100)\n",
    "print(lstm_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting pre trained embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['AFP', '-', 'Sudan', 'has', 'condemned', 'as', 'quot', ';', 'unfair', 'quot', ';', 'a', 'new', 'UN', 'resolution', 'calling', 'on', 'Khartoum', 'to', 'restore', 'security', 'to', 'the', 'crisis-wracked', 'Darfur', 'region', 'or', 'face', 'possible', 'sanctions', ',', 'but', 'said', 'it', 'would', 'abide', 'by', 'the', 'UN', \"'s\", 'demands', '.']]\n",
      "5021\n"
     ]
    }
   ],
   "source": [
    "# wor2vec_model = gensim.downloader.load('word2vec-google-news-300')#word2vec-google-news-300\n",
    "vocab = build_vocab_from_iterator([nltk.word_tokenize(i) for i in df['Description']], specials=[PAD_TOKEN, UNKNOWN_TOKEN, START_TOKEN, END_TOKEN])\n",
    "vocab.set_default_index(vocab[UNKNOWN_TOKEN])\n",
    "print([nltk.word_tokenize(df['Description'][2054])])\n",
    "print(vocab['Khartoum'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# embedding_matrix = torch.zeros(len(vocab), wor2vec_model.vector_size)\n",
    "# for i, word in enumerate(vocab.get_itos()):\n",
    "#     if word in wor2vec_model:\n",
    "#         embedding_matrix[i] = torch.tensor(wor2vec_model[word])\n",
    "#     else:\n",
    "#         embedding_matrix[i] = torch.tensor(wor2vec_model[\"unk\"])\n",
    "#change this to random later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21822\n"
     ]
    }
   ],
   "source": [
    "print(vocab['hello'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "def collate(batch):\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    for i in range(len(batch)):\n",
    "\n",
    "        inputs.append(torch.tensor([vocab[word] if word in vocab else vocab[UNKNOWN_TOKEN] for word in batch[i][0]]))\n",
    "        targets.append(torch.tensor([vocab[word] if word in vocab else vocab[UNKNOWN_TOKEN] for word in batch[i][1]]))\n",
    "    return pad_sequence(inputs, batch_first=True, padding_value=vocab[PAD_TOKEN]),pad_sequence(targets, batch_first=True, padding_value=vocab[PAD_TOKEN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_data_loader = DataLoader(lstm_data, batch_size=32, shuffle=True, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238\n"
     ]
    }
   ],
   "source": [
    "print(len(lstm_data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    2,  7495,  5873,  ...,     0,     0,     0],\n",
      "        [    2,   102,   110,  ...,     0,     0,     0],\n",
      "        [    2,   319,    14,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [    2,   224,  4193,  ...,     0,     0,     0],\n",
      "        [    2,    52,    14,  ...,     0,     0,     0],\n",
      "        [    2, 17588,   243,  ...,    34,    62,    12]])\n",
      "tensor([[ 7495,  5873,  1634,  ...,     0,     0,     0],\n",
      "        [  102,   110,    21,  ...,     0,     0,     0],\n",
      "        [  319,    14,    42,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  224,  4193, 20338,  ...,     0,     0,     0],\n",
      "        [   52,    14,    20,  ...,     0,     0,     0],\n",
      "        [17588,   243,   728,  ...,    62,    12,     3]])\n",
      "<s>\n",
      "5021\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    for inputs, targets in lstm_data_loader:\n",
    "        print(inputs)\n",
    "        print(targets)\n",
    "        break\n",
    "print(vocab.get_itos()[2])\n",
    "print(vocab['Khartoum'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ELMO architecture.\n",
    "2 layer bi lstm as (flstm1,rlstm1,flstm2,rlstm2)\n",
    "Returns these values, which are then concatenated and made of same size by padding (or duplicating)\n",
    "Leanable parameters have been done using the learnable parameters of pytorch, where 3 values were learned.(one for each bilstm layer and 1 for pretrained embedding)\n",
    "Used required_grad flag to freeze the elmo model and trained downstream.\n",
    "for func, i used an nn.Linear()\n",
    "and random with np.random.\n",
    "All accuracy values are shown in report.\n",
    "\n",
    "Learning - 0.71\n",
    "Fixed(random) - 0.64\n",
    "Function - 0.76\n",
    "\n",
    "We see for random the lowest, but not very bad compared to others, which is to be expected.\n",
    "For Function we have more complex linear transforms to learn hence the increase from normal learnable parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LM(nn.Module):\n",
    "    def __init__(self,vocab,batch_size, hidden_size,output_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(len(vocab), hidden_size)\n",
    "        self.flstm1 = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.flstm2 = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.rlstm1 = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.rlstm2 = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.output = nn.Linear(2*hidden_size, output_size)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        o_f1,_ = self.flstm1(x)\n",
    "        o_f2,_ = self.flstm2(o_f1)\n",
    "        o_r1,_ = self.rlstm1(torch.flip(x, [1]))\n",
    "        o_r2,_ = self.rlstm2(o_r1)\n",
    "        return x,o_f1,o_f2,o_r1,o_r2\n",
    "    \n",
    "    def elmo(self, x,batch_size):\n",
    "        x,of1,of2,or1,or2 = self.forward(x)\n",
    "        or2 = torch.flip(or2, dims=(1,))\n",
    "        cat = torch.cat((of2,or2),axis=2)\n",
    "        return self.output(cat)\n",
    "\n",
    "\n",
    "LanguageModel = LM(vocab=vocab,\n",
    "                batch_size=32,\n",
    "               hidden_size=512,\n",
    "               output_size=len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1856]) torch.Size([1856, 25974])\n",
      "torch.Size([2592]) torch.Size([2592, 25974])\n",
      "torch.Size([1504]) torch.Size([1504, 25974])\n",
      "torch.Size([2720]) torch.Size([2720, 25974])\n",
      "torch.Size([1824]) torch.Size([1824, 25974])\n",
      "torch.Size([2592]) torch.Size([2592, 25974])\n",
      "torch.Size([2368]) torch.Size([2368, 25974])\n",
      "torch.Size([4800]) torch.Size([4800, 25974])\n",
      "torch.Size([1760]) torch.Size([1760, 25974])\n",
      "torch.Size([1632]) torch.Size([1632, 25974])\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "# Lists to store training and validation losses\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "LanguageModel.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(LanguageModel.parameters(), lr=0.1)\n",
    "c = 10\n",
    "for epoch in range(num_epochs):\n",
    "    LanguageModel.train()\n",
    "    epoch_train_loss = 0.0\n",
    "    for batch in lstm_data_loader:\n",
    "        inputs, labels = batch\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = LanguageModel.elmo(inputs,inputs.shape[0])\n",
    "        outputs = outputs.view(-1, outputs.shape[-1])\n",
    "        labels = labels.view(-1)\n",
    "        print(labels.shape,outputs.shape)\n",
    "        break\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_train_loss += loss.item()\n",
    "        print(loss.item())\n",
    "        c-=1\n",
    "        if c < 0:\n",
    "            break\n",
    "    epoch_train_loss /= len(lstm_data_loader)\n",
    "    train_loss.append(epoch_train_loss)\n",
    "    LanguageModel.eval()\n",
    "    epoch_val_loss = 0.0\n",
    "    torch.save(LanguageModel, f\"elmo.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fannie Fannie\n",
      "The Big\n",
      "A on from from that\n",
      "A Fort Worth brokerage that\n",
      "Fannie failure \\ of a\n"
     ]
    }
   ],
   "source": [
    "LanguageModel = torch.load(\"elmo.pt\", map_location=device)\n",
    "LanguageModel.eval()\n",
    "vocab_list = vocab.get_itos()\n",
    "for batch in lstm_data_loader:\n",
    "    inputs, labels = batch\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    outputs = LanguageModel.elmo(inputs,inputs.shape[0])\n",
    "    print(vocab_list[torch.argmax(outputs[0][0])],vocab_list[torch.argmax(outputs[0][1])])\n",
    "    print(vocab_list[labels[0][0]],vocab_list[labels[0][1]])\n",
    "    print(vocab_list[torch.argmax(outputs[1][0])],vocab_list[torch.argmax(outputs[1][1])],vocab_list[torch.argmax(outputs[1][2])],vocab_list[torch.argmax(outputs[1][3])],vocab_list[torch.argmax(outputs[1][4])])\n",
    "    print(vocab_list[labels[1][0]],vocab_list[labels[1][1]],vocab_list[labels[1][2]],vocab_list[labels[1][3]],vocab_list[labels[1][4]])\n",
    "    print(vocab_list[torch.argmax(outputs[2][0])],vocab_list[torch.argmax(outputs[2][1])],vocab_list[torch.argmax(outputs[2][2])],vocab_list[torch.argmax(outputs[2][3])],vocab_list[torch.argmax(outputs[2][4])])\n",
    "    break   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['<s>', 'unions', 'representing', 'workers', 'at', 'turner', 'newall', 'say', 'they', 'are', \"'disappointed\", \"'\", 'after', 'talks', 'with', 'stricken', 'parent', 'firm', 'federal', 'mogul', '</s>'], 3)\n"
     ]
    }
   ],
   "source": [
    "class DownStreamDataset:\n",
    "    def __init__(self, dataset, tokenizer, max_length):\n",
    "        self.dataset = dataset['Description']\n",
    "        self.labels = dataset['Class Index']\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        tokenized_sentence = self.tokenizer(re.sub(r'[^\\w\\s\\']', '',self.dataset[idx].lower()))\n",
    "        return [START_TOKEN]+tokenized_sentence+[END_TOKEN],self.labels[idx]\n",
    "\n",
    "         \n",
    "Down_data = DownStreamDataset(df,nltk.word_tokenize, 100)\n",
    "print(Down_data[0])\n",
    "def collate(batch):\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    for i in range(len(batch)):\n",
    "        inputs.append(torch.tensor([vocab[word] if word in vocab else vocab[UNKNOWN_TOKEN] for word in batch[i][0]]))\n",
    "        targets.append(batch[i][1])\n",
    "    targets = torch.tensor(targets)\n",
    "    return pad_sequence(inputs, batch_first=True, padding_value=vocab[PAD_TOKEN]),targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 56])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "Down_data_loader = DataLoader(Down_data, batch_size=32, shuffle=True, collate_fn=collate)\n",
    "for i in range(1):\n",
    "    for inputs, targets in Down_data_loader:\n",
    "        print(inputs.shape)\n",
    "        print(targets.shape)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownStreamClassifier(torch.nn.Module):\n",
    "  def __init__(self,num_label,elmo,hidden_dim,embed_dim,num_layers):\n",
    "    super().__init__()\n",
    "\n",
    "    \n",
    "    self.lstm = torch.nn.LSTM(embed_dim,hidden_dim,num_layers,batch_first=True,bidirectional=True)\n",
    "    self.outLayer = torch.nn.Linear(2*hidden_dim,num_label)\n",
    "    self.num_classes = num_label\n",
    "    self.elmo = elmo\n",
    "    self.embed_func = torch.nn.Linear(6*hidden_dim,embed_dim)\n",
    "    self.elmo.to(device)\n",
    "\n",
    "  def forward(self, sentence):\n",
    "    self.elmo.eval()\n",
    "    x,of1,of2,or1,or2 = self.elmo.forward(sentence.to(device))\n",
    "    first_layer = torch.cat((of1,or1),axis=2)\n",
    "    second_layer = torch.cat((of2,or2),axis=2)\n",
    "    x = torch.cat((x,x),axis=2)\n",
    "    embedding = self.embed_func(torch.cat((first_layer,second_layer,x),axis=2))\n",
    "    lstm_out,_ = self.lstm(embedding)\n",
    "    lstm_out = lstm_out[:,-1,:]\n",
    "    output = self.outLayer(lstm_out)\n",
    "    return torch.nn.functional.softmax(output,dim=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238\n",
      "lambdas torch.Size([3])\n",
      "lstm.weight_ih_l0 torch.Size([512, 1024])\n",
      "lstm.weight_hh_l0 torch.Size([512, 128])\n",
      "lstm.bias_ih_l0 torch.Size([512])\n",
      "lstm.bias_hh_l0 torch.Size([512])\n",
      "lstm.weight_ih_l0_reverse torch.Size([512, 1024])\n",
      "lstm.weight_hh_l0_reverse torch.Size([512, 128])\n",
      "lstm.bias_ih_l0_reverse torch.Size([512])\n",
      "lstm.bias_hh_l0_reverse torch.Size([512])\n",
      "outLayer.weight torch.Size([4, 256])\n",
      "outLayer.bias torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "print(len(Down_data_loader))\n",
    "DownStreamModel = DownStreamClassifier(4,LanguageModel,128,512*2,1)\n",
    "for name, param in DownStreamModel.named_parameters():\n",
    "    if name.split('.')[0] == 'elmo':\n",
    "        param.requires_grad = False\n",
    "for name, param in DownStreamModel.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name,param.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_num=0, loss.item()=-0.24496591091156006\n",
      "batch_num=1, loss.item()=-0.24247388541698456\n",
      "batch_num=2, loss.item()=-0.24708682298660278\n",
      "batch_num=3, loss.item()=-0.25897201895713806\n",
      "batch_num=4, loss.item()=-0.253031849861145\n",
      "batch_num=5, loss.item()=-0.2507251501083374\n",
      "batch_num=6, loss.item()=-0.2511810064315796\n",
      "batch_num=7, loss.item()=-0.2567189931869507\n",
      "batch_num=8, loss.item()=-0.24696293473243713\n",
      "batch_num=9, loss.item()=-0.2605348229408264\n",
      "batch_num=10, loss.item()=-0.24969173967838287\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch_num \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m     12\u001b[0m   \u001b[38;5;66;03m# Put model in training mode\u001b[39;00m\n\u001b[1;32m     13\u001b[0m   DownStreamModel\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 14\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m batch_num, (sentences, tags) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(Down_data_loader):\n\u001b[1;32m     15\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m sentences\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     16\u001b[0m     tags \u001b[38;5;241m=\u001b[39m tags\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 22\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     20\u001b[0m targets \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(batch)):\n\u001b[0;32m---> 22\u001b[0m     inputs\u001b[38;5;241m.\u001b[39mappend(torch\u001b[38;5;241m.\u001b[39mtensor([vocab[word] \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m vocab \u001b[38;5;28;01melse\u001b[39;00m vocab[UNKNOWN_TOKEN] \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m batch[i][\u001b[38;5;241m0\u001b[39m]]))\n\u001b[1;32m     23\u001b[0m     targets\u001b[38;5;241m.\u001b[39mappend(batch[i][\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     24\u001b[0m targets \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(targets)\n",
      "Cell \u001b[0;32mIn[13], line 22\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     20\u001b[0m targets \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(batch)):\n\u001b[0;32m---> 22\u001b[0m     inputs\u001b[38;5;241m.\u001b[39mappend(torch\u001b[38;5;241m.\u001b[39mtensor([vocab[word] \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mword\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m \u001b[38;5;28;01melse\u001b[39;00m vocab[UNKNOWN_TOKEN] \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m batch[i][\u001b[38;5;241m0\u001b[39m]]))\n\u001b[1;32m     23\u001b[0m     targets\u001b[38;5;241m.\u001b[39mappend(batch[i][\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     24\u001b[0m targets \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(targets)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchtext/vocab/vocab.py:54\u001b[0m, in \u001b[0;36mVocab.__contains__\u001b[0;34m(self, token)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mexport\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__contains__\u001b[39m(\u001b[38;5;28mself\u001b[39m, token: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m     47\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m        token: The token for which to check the membership.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;124;03m        Whether the token is member of vocab or not.\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__contains__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "# lstm_test_dataloader = DataLoader(lstm_val_dataset, batch_size=3,collate_fn=lstm_data.collate)\n",
    "DownStreamModel.float()\n",
    "DownStreamModel.to(device)\n",
    "loss_fn = torch.nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(DownStreamModel.parameters(),lr = 0.01)\n",
    "\n",
    "for epoch_num in range(10):\n",
    "  # Put model in training mode\n",
    "  DownStreamModel.train()\n",
    "  for batch_num, (sentences, tags) in enumerate(Down_data_loader):\n",
    "    sentences = sentences.to(device)\n",
    "    tags = tags.to(device)\n",
    "    \n",
    "    pred = DownStreamModel(sentences)\n",
    "    loss = loss_fn(pred,tags-1)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    " \n",
    "    print(f\"{batch_num=}, {loss.item()=}\")\n",
    "      \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) tensor([0, 3, 0, 0, 3, 3, 3, 2, 1, 2, 1, 3, 3, 0, 1, 1, 0, 0, 3, 0, 3, 2, 0, 2,\n",
      "        0, 3, 1, 0, 2, 1, 1, 3, 3, 3, 3, 0, 0, 0, 0, 1, 2, 3, 3, 1, 3, 2, 1, 3,\n",
      "        1, 0, 1, 2, 3, 3, 3, 1, 3, 0, 2, 0, 0, 1, 1, 3])\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2]) tensor([3, 0, 3, 3, 0, 3, 1, 1, 3, 0, 2, 3, 3, 3, 0, 3, 3, 0, 0, 3, 3, 1, 3, 3,\n",
      "        3, 2, 0, 0, 1, 1, 1, 0, 0, 0, 0, 2])\n",
      "[0, 3, 0, 0, 3, 3, 3, 2, 1, 2, 1, 3, 3, 0, 1, 1, 0, 0, 3, 0, 3, 2, 0, 2, 0, 3, 1, 0, 2, 1, 1, 3, 3, 3, 3, 0, 0, 0, 0, 1, 2, 3, 3, 1, 3, 2, 1, 3, 1, 0, 1, 2, 3, 3, 3, 1, 3, 0, 2, 0, 0, 1, 1, 3, 3, 0, 3, 3, 0, 3, 1, 1, 3, 0, 2, 3, 3, 3, 0, 3, 3, 0, 0, 3, 3, 1, 3, 3, 3, 2, 0, 0, 1, 1, 1, 0, 0, 0, 0, 2] tensor([2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
      "Accuracy: 0.14\n",
      "Recall: 0.14\n",
      "F1 Score: 0.05828092243186583\n",
      "Confusion Matrix:\n",
      "[[ 0  1 29  0]\n",
      " [ 0  2 19  0]\n",
      " [ 0  0 12  0]\n",
      " [ 0  3 34  0]]\n"
     ]
    }
   ],
   "source": [
    "csv_file_test = 'iNLP-A2/ANLP-2/test.csv'\n",
    "tf = pd.read_csv(csv_file_test)\n",
    "tf = tf.head(100)\n",
    "test_data = DownStreamDataset(tf,nltk.word_tokenize, 100)\n",
    "test_data_loader = DataLoader(test_data, batch_size=64, shuffle=True,collate_fn=collate)\n",
    "\n",
    "DownStreamModel.eval()\n",
    "\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "# Iterate over test set\n",
    "for sentences, labels in test_data_loader:\n",
    "    # Forward pass\n",
    "    output = DownStreamModel(sentences)\n",
    "    labels = labels-1\n",
    "\n",
    "    # Get predicted labels\n",
    "    \n",
    "    _, predicted = torch.max(output, 1)\n",
    "    print(predicted,labels)\n",
    "    \n",
    "    # Append true and predicted labels\n",
    "    true_labels.extend(labels.view(-1).tolist())\n",
    "    predicted_labels.extend(predicted.tolist())\n",
    "\n",
    "# Compute evaluation metrics\n",
    "print(true_labels,predicted)\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "recall = recall_score(true_labels, predicted_labels,average=\"weighted\")\n",
    "f1 = f1_score(true_labels, predicted_labels,average=\"weighted\")\n",
    "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
